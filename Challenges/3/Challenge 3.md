# Conceptos clave en SVM: C, Kernel y Gamma

## ğŸ›ï¸ HiperparÃ¡metro `C` en SVM

### ğŸ§© Â¿QuÃ© es `C`?

`C` es el **parÃ¡metro de regularizaciÃ³n** en el algoritmo SVM.  
Determina **cuÃ¡nto penalizamos los errores** cometidos al clasificar los datos de entrenamiento.

---

### ğŸ“ IntuiciÃ³n geomÃ©trica

El objetivo del SVM es encontrar un **hiperplano con el mayor margen** que separe las clases, pero a veces no se puede separar perfectamente sin cometer errores.

- `C` controla el **equilibrio entre el margen ancho y la clasificaciÃ³n correcta** de cada punto de entrenamiento.

---

### ğŸ” Â¿QuÃ© pasa al modificar `C`?

| Valor de `C` | Comportamiento del modelo                                   | Riesgo            |
|--------------|-------------------------------------------------------------|-------------------|
| ğŸ”¹ **C bajo**  | Acepta mÃ¡s errores. Busca un margen amplio.                | â– Underfitting    |
| ğŸ”¸ **C alto**  | Penaliza fuertemente los errores. Intenta clasificar todo bien. | â• Overfitting     |

---

### ğŸ¯ AnalogÃ­a simple

Imagina que entrenas a un robot que debe separar pelotas rojas y azules con una regla:

- Si le das un **C bajo**, le dices: *"No importa si te equivocas un poco, pero asegÃºrate de hacer una separaciÃ³n simple"*.
- Si le das un **C alto**, le dices: *"Â¡Prohibido equivocarse! Clasifica TODO bien aunque tengas que hacer una frontera rara o enredada"*.

---

### ğŸ“Š VisualizaciÃ³n conceptual

> AquÃ­ podrÃ­as incluir una imagen con dos subgrÃ¡ficos que comparen:

1. **C = 0.1**: Margen amplio, acepta algunos errores.
2. **C = 100**: Margen estrecho, ajusta demasiado al conjunto de entrenamiento.

---

### ğŸ§ª CÃ³digo de ejemplo

```python
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

# Dataset sintÃ©tico para visualizaciÃ³n
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=100, n_features=2, 
                           n_redundant=0, n_informative=2,
                           random_state=42, n_clusters_per_class=1)

# Entrenar modelos con diferentes valores de C
model_lowC = SVC(kernel='linear', C=0.1)
model_highC = SVC(kernel='linear', C=100)

model_lowC.fit(X, y)
model_highC.fit(X, y)

# FunciÃ³n para graficar el resultado
def plot_decision_boundary(model, title):
    w = model.coef_[0]
    b = model.intercept_[0]
    x_plot = np.linspace(X[:, 0].min(), X[:, 0].max(), 30)
    y_plot = -(w[0] * x_plot + b) / w[1]

    plt.figure()
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolors='k')
    plt.plot(x_plot, y_plot, 'k-')
    plt.title(title)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.grid(True)
    plt.show()

plot_decision_boundary(model_lowC, "SVM con C=0.1 (margen amplio)")
plot_decision_boundary(model_highC, "SVM con C=100 (margen estrecho)")


## ğŸŒ Â¿QuÃ© es el Kernel?

El **kernel** es una funciÃ³n que permite que un modelo SVM (Support Vector Machine) **proyecte los datos a un espacio de mayor dimensiÃ³n** para encontrar una frontera de separaciÃ³n mÃ¡s clara entre clases, incluso cuando no es posible separarlas linealmente en su espacio original.

### ğŸ¯ Â¿Para quÃ© sirve el kernel?
- Transforma los datos para que **sÃ­ puedan separarse con una lÃ­nea recta (hiperplano)**.
- Permite capturar **relaciones no lineales** entre los datos.
- Evita que el modelo tenga que hacer cÃ¡lculos complejos en espacios de muchas dimensiones (gracias al "truco del kernel").

### ğŸ§ª Tipos comunes de kernel:
| Kernel        | CaracterÃ­sticas                                | Ideal para...                              |
|---------------|-------------------------------------------------|---------------------------------------------|
| `linear`      | SeparaciÃ³n lineal directa                       | Datos que ya son linealmente separables     |
| `poly`        | Usa polinomios de grado n                      | Relaciones mÃ¡s complejas                   |
| `rbf`         | FunciÃ³n gaussiana (Radial Basis Function)       | Fronteras suaves y no lineales              |
| `sigmoid`     | Similar a redes neuronales                      | Usado en ciertos contextos especÃ­ficos      |

---

## âš™ï¸ Â¿QuÃ© es Gamma (`Î³`)?

`gamma` es un **hiperparÃ¡metro** que se utiliza en los **kernels no lineales**, como `rbf`, `poly` o `sigmoid`. Controla **cuÃ¡nto influye cada punto de entrenamiento** sobre la forma de la frontera de decisiÃ³n del modelo.

### ğŸ§  IntuiciÃ³n:
> `gamma` define el **alcance de la memoria** de cada punto.  
> Â¿Ve solo su cuadra o ve toda la ciudad?

### ğŸ” Â¿CÃ³mo afecta el valor de gamma?

| Valor de `gamma` | Comportamiento del modelo                    | Posible problema      |
|------------------|-----------------------------------------------|------------------------|
| Bajo (`â†“`)       | Cada punto influye en una **zona amplia**     | Underfitting           |
| Medio (Ã³ptimo)   | Frontera curva y **bien ajustada**            | Buen rendimiento       |
| Alto (`â†‘`)       | Cada punto solo influye en su **vecindad**    | Overfitting            |

### ğŸ“Œ Recomendaciones prÃ¡cticas:
- `gamma` solo aplica a kernels **no lineales**.
- Para un buen modelo, debe ajustarse junto con otros hiperparÃ¡metros (como `C`).
- Una gamma muy alta puede hacer que el modelo aprenda **ruido o detalles irrelevantes**.

---

## ğŸ§ª ComparaciÃ³n grÃ¡fica (imaginaria)

![image](https://github.com/user-attachments/assets/243327d8-f256-4992-bfd5-f71ec4ec6533)

ğŸ” ExplicaciÃ³n del grÃ¡fico:

Gamma bajo (0.01):
La frontera es muy suave, generaliza demasiado â†’ underfitting.

Gamma medio (1):
La frontera se ajusta bien a los datos â†’ buen ajuste.

Gamma alto (100):
La frontera es muy irregular y compleja â†’ overfitting (el modelo memoriza el ruido).
